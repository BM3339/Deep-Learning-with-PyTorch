{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7fef07",
   "metadata": {},
   "source": [
    "Name: Bharat Modhwadiya\n",
    "bm3339@drexel.edu\n",
    "(267)312-3546"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4da23",
   "metadata": {},
   "source": [
    "# Introduction to deep learning with PyTorch\n",
    "\n",
    "What is deep learning?\n",
    "Deep learning is everywhere:\n",
    "• Language translation\n",
    "• Self-driving cars\n",
    "• Medical diagnostics\n",
    "• Chatbots\n",
    "• Used on multiple data types: images, text and audio\n",
    "\n",
    "**Traditional machine learning: relies on hand-crafted feature engineering\n",
    "### Where Deep Learning enables feature learning from raw data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647b635",
   "metadata": {},
   "source": [
    "Deep learning is a subset of machine learning\n",
    "• Inspired by connections in the human brain\n",
    "• Models require large amount of data\n",
    "\n",
    "PyTorch: a deep learning framework\n",
    "PyTorch is one of the most popular deep learning frameworks \n",
    "• the framework used in many published deep learning papers\n",
    "• intuitive and user-friendly\n",
    "• has much in common with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cb6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imporiting PyTorch and related package\n",
    "\n",
    "import torch\n",
    "# Image data with torchvision\n",
    "# Audio data with torchaudio\n",
    "# Text data with torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e39d19",
   "metadata": {},
   "source": [
    "Fundamental data structure in PyTorch is called 'Tensors'\n",
    "Tensors : the building bloack of Networks in PyTorch\n",
    "\n",
    "#### A tensor is a mathematical object representing a multi-dimensional array of numerical values.\n",
    "\n",
    "we can load torch from list and NumPy array \n",
    "\n",
    "lst = [[1,2,3],[4,5,6]]\n",
    "tensor = torch.tensor(lst)\n",
    "\n",
    "or\n",
    "\n",
    "np_array = np.array(array)\n",
    "np_tensor = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8afc0d",
   "metadata": {},
   "source": [
    "### Tensor Attributes \n",
    "\n",
    "• Tensor Shape : to check the size of created tensor tensor.shape\n",
    "• Tensor data type - tensor.dtype\n",
    "• Tensor Device - tensor.device\n",
    "\n",
    "Deep Learning often requires a GPU, which compared to a CPU can offer:\n",
    "• Parralel Computing capabilities\n",
    "• Faster training times\n",
    "• Better Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e82ad",
   "metadata": {},
   "source": [
    "Most NumPy array Operations can be performed on PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604b19e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "print(tensor_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0ef3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Display the tensor device\n",
    "print(tensor_a.device)\n",
    "\n",
    "# Display the tensor data type\n",
    "print(tensor_a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127706a3",
   "metadata": {},
   "source": [
    "Creating tensors from NumPy arrays\n",
    "Tensors are the fundamental data structure of PyTorch. You can create complex deep learning algorithms by learning how to manipulate them.\n",
    "\n",
    "The torch package has been imported, and two NumPy arrays have been created, named array_a and array_b. Both arrays have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45203d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tensors from the arrays\n",
    "tensor_a = torch.tensor(array_a)\n",
    "tensor_b = torch.tensor(array_b)\n",
    "\n",
    "# Subtract tensor_b from tensor_a \n",
    "tensor_c = tensor_a - tensor_b\n",
    "\n",
    "# Multiply each element of tensor_a with each element of tensor_b\n",
    "tensor_d = tensor_a*tensor_b\n",
    "\n",
    "# Add tensor_c with tensor_d\n",
    "tensor_e = tensor_c+tensor_d\n",
    "print(tensor_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cbbf55",
   "metadata": {},
   "source": [
    "## Creating our first neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a6829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "## Create input_tensor with three features\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547,-0.235611]])\n",
    "\n",
    "#define our first Linear layer \n",
    "linear_layer = nn.Linear(in_features = 3, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54bcd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1320, -0.5357]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Pass input through linear layer\n",
    "\n",
    "output = linear_layer(input_tensor)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7018aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Linear layer has weight and bias property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecd36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1236, -0.4734,  0.4159],\n",
      "        [ 0.1976, -0.4156,  0.3515]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1384, -0.3325], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3f4ad",
   "metadata": {},
   "source": [
    "Getting to know the linear layer operation\n",
    "output = linear_layer(input_tensor)\n",
    "For input X, weights W0 and bias b0, the linear layer performs\n",
    "yo = W0*X + b0\n",
    "In PyTorch: output = W0 @ input + b0\n",
    "#### Weights and biases are initialized randomly\n",
    "• They are not useful until they are tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63ebbb61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0948]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## USE of nn.Sequential\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8,1),\n",
    "                      nn.Linear(1,1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5964e",
   "metadata": {},
   "source": [
    "#### In practice, you'll find that modern neural networks can contain hundreds of layers and millions of parameters. Recall that the model output is not meaningful until the model is trained, i.e. until the weights and biases of each layer can meaningfully be used to produce output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b20085",
   "metadata": {},
   "source": [
    "### Discovering activation functions\n",
    "\n",
    "Till time we have seen linear operation and linear output. But we can create non-linear output as well using activation function. Activation functions adds non-linearity to the network.\n",
    "#### pre-activation output and output after activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080a3d2",
   "metadata": {},
   "source": [
    "### SIGMOID FUNCTION\n",
    "\n",
    "### Used for Binary classification task. \n",
    "\n",
    "Let's say we have 3 input as limbs(4), layseggs?(0), have hair? (1).. Now we want to that is that animal is mamal or not. \n",
    "To predict whether animal is 1 (mammal) or O (not mammal),\n",
    "• we got the pre-activation (6),\n",
    "now pass it to the sigmoid, and obtain a value between 0 and 1.\n",
    "Using the common threshold of 0.5:\n",
    "• If output is > 0.5, class label = 1 (mammal)\n",
    "• If output is <= 0.5, class label = 0 (not mammal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69812205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9975]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[6.0]])\n",
    "Sigmoid = nn.Sigmoid()\n",
    "output = Sigmoid(input_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e18c82",
   "metadata": {},
   "source": [
    "Sigmoid is used as last step when using nn.Sequential to check for binary classification task.\n",
    "\n",
    "#### What can be used for Multi Class Classification Problem ??\n",
    "## INTRODUCINT 'SOFTMAX'\n",
    "\n",
    "say N=3 classes:\n",
    "• bird (0), mammal (1), reptile (2)\n",
    "• output has three elements, so softmax has three elements\n",
    "• outputs a probability distribution:\n",
    "• each element is a probability (it's bounded between 0 and 1)\n",
    "• the sum of the output vector is equal to 1\n",
    "\n",
    "### probabilities = nn.Softmax(dim=-1)\n",
    "### output_tensor = probabilities(input_tensor)\n",
    "### dim = -1 indicates softmax is applied to the input tensor's last dimension\n",
    "### nn. Sigmoid() can be used as last step in nn. Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75390558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f844e",
   "metadata": {},
   "source": [
    "### Running a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3c0c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8681]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8,1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd60b57",
   "metadata": {},
   "source": [
    "The sigmoid output cannot return any float value: the output returned by your binary classifier is bounded between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7078c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1510, 0.3198, 0.0847, 0.4446]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Practice \n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd93bd",
   "metadata": {},
   "source": [
    "Why do we need a loss function ?\n",
    "How do we know that predection values are acturate or it has errors. \n",
    "\n",
    "Loss function gives feedback to model during training. Our goal is to minimise the loss.\n",
    "\n",
    "Loss = F(y,ý) ý is Y hat\n",
    "\n",
    "y is single integer (class label) 0 or 1.\n",
    "\n",
    "ý is Tensor (output of Softmax function)\n",
    "Let's N=3. ý = [0.57492, 0.034961, 0.156691]\n",
    "\n",
    "HOW do we compare an integer with a tensor ??\n",
    "\n",
    "## ONE-HOT Encoding is used now\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "F.one_hot(torch.tensor(0),num_classes=3)\n",
    "\n",
    "## CROSS ENTROPY LOSS in PyTorch\n",
    "it is the most used loss function in classification problem in pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5eb35c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=4)\n",
    "one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1b0a3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(),one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead01561",
   "metadata": {},
   "source": [
    "our goal is to minimise the loss. Higher the loss, bad the model is. So derivaties are used to minimize the loss the loss function\n",
    "## Using derivatives to update model parameters\n",
    "\n",
    "we calculate the Gradient of loss function and use that to train the model using backpropogation. so that weight and bias are assgined properly.\n",
    "\n",
    "loss function used in deep learning is non-convex. \n",
    "to find the global minimum of non-convex, we use mechnism called 'Gradient descent'\n",
    "\n",
    "In PyTorch,an Optimizer takes care of Weight updates. \n",
    "\n",
    "Most common Optimizer is SGD (stochastic gradient descent) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters), lr=0.001)\n",
    "\n",
    "## Optimizer handles updating model parameters (or weights) after calculation of local gradients\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias\n",
    "\n",
    "##Without PyTorch\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr*grads0\n",
    "weight1 = weight1 - lr*grads1\n",
    "weight2 = weight2 - lr*grads2\n",
    "\n",
    "\n",
    "#This is too Tedious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae66fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()\n",
    "\n",
    "##SGD is only one of the many optimizers implemented in PyTorch. \n",
    "##Researchers keep on improving the optimization process for training.\n",
    "\n",
    "##CrossEntropyLoss function is only used for classification problem. \n",
    "\n",
    "## criterion = nn.MSEloss()\n",
    "# loss = criterion(prediction, target) where prediction and target must be float tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14939b8e",
   "metadata": {},
   "source": [
    "## Writing our first training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbbc1605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y) ** 2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d598ba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we will write a training loop every time we train a deep learning model with PyTorch.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Loop over the number of epochs and the dataloader\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Set the gradients to zero\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# we will write a training loop every time we train a deep learning model with PyTorch.\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "show_results(model, dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1244da1",
   "metadata": {},
   "source": [
    "## Discovering activation functions between layers\n",
    "Sigmoid and SoftMax functions are used at last layer. Sigmoid and Softmax functions are varies between 0 and 1 and it create lot's limitation in training the network challenging. it's also called Vanishing Gradient. \n",
    "\n",
    "## Introducing ReLU\n",
    "\n",
    "F(x) = Max(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd05cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "leaky_relu = nn.LeakyReLU(negative_slope = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)\n",
    "\n",
    "##Notice that the input value was zero, and the ReLU function turns any negative input to zero. \n",
    "##Recall from the graph in the video that for negative values of x, the output of ReLU is always zero, \n",
    "## and indeed the gradient is zero everywhere \n",
    "## because there is no change in the function with respect to any negative value of x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
